# -*- coding: utf-8 -*-
"""19201055carprice_without sklearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxnkq7aN1zoW51_52jTAiJZjWW8x5Uid
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt


from sklearn.tree import DecisionTreeRegressor
from sklearn import datasets

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
from typing_extensions import Text
data = pd.read_csv('/content/drive/MyDrive/ai/CarPrice_Assignment.csv')
df = data.copy()

df.head()

m=len(data) #Length Of Dataset
print(m)

data.info()

x1=data["car_ID"]
x2=data["symboling"]
x3=data["CarName"]
x4=data["wheelbase"]
x5=data["carlength"]
x6=data["carwidth"]
x7=data["carheight"]
x8=data["curbweight"]
x9=data["enginesize"]
x10=data["stroke"]
x11=data["horsepower"]
x12=data["highwaympg"]
y=data["price"]
print(x1.tail())
print(x2.head())
print(x3.head())
print(x4.head())
print(x5.head())
print(x6.head())
print(x7.head())
print(x8.head())
print(x9.head())
print(x10.head())
print(x11.head())
print(x12.head())
print(y.head())
len(x1)

from sklearn import preprocessing
n1=preprocessing.normalize([x1])
n2=preprocessing.normalize([x2])
n4=preprocessing.normalize([x4])
n5=preprocessing.normalize([x5])
n6=preprocessing.normalize([x6])
n7=preprocessing.normalize([x7])
n8=preprocessing.normalize([x8])
n9=preprocessing.normalize([x9])
n10=preprocessing.normalize([x10])
n11=preprocessing.normalize([x11])
n12=preprocessing.normalize([x12])
print(n1,n2,n4,n5,n6,n7,n8,n9,n10,n11,n12)

x1=n1[0]
x2=n2[0]
x4=n4[0]
x5=n5[0]
x6=n6[0]
x7=n7[0]
x8=n8[0]
x9=n9[0]
x10=n10[0]
x11=n11[0]
x12=n12[0]
print(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12)

alpha=0.1
print(alpha)

J = 100
n = 0 # Iteration Number
theta = [1,1.5, 8,9]
cost_ls=[]

for i in range(100):

  print("Iteration number: ",n+1)
  n = n+1
  # Hypothesis Function
  h = []
  print("Hypothesis function value is: h0(x)=theta_0+theta_1 * x")
  for i2 in range(m):
    temp = theta[0] + theta[1]*x1[i2] + theta[2]*x2[i2]+theta[3]*x3[i2]
    h.append(temp)

  # Cost Function
  error_sum = 0
  print("Cost function is: j(theta)=1/(2*m) * i=1_samtionSign_m (h_theta_(x)-y)**2")

  for i3 in range(m):
    error_sum = error_sum + (h[i3] - y[i3])**2

  J = (1/(2*m))*error_sum
  if J == float("inf") :
    theta = [random.randint(-100,100),random.randint(-100,100), random.randint(-100,100),random.randint(-100,100)]
    continue
  print("\nCost function is:",J)
  cost_ls.append(J)

    # Gradient Descent
  print("\ngradient decent:")
  temp0 = 0
  for i4 in range(m):
    temp0 = temp0 + (h[i4] - y[i4])

  theta[0] = theta[0] - (alpha/m)*temp0

  temp1 = 0
  for i5 in range(m):
    temp1 = temp1 + (h[i5] - y[i5])*x1[i5]

  theta[1] = theta[1] - (alpha/m)*temp1

  temp1 = 0
  for i5 in range(m):
    temp1 = temp1 + (h[i5] - y[i5])*x2[i5]

  theta[2] = theta[2] - (alpha/m)*temp1

  temp1 = 0
  for i5 in range(m):
    temp1 = temp1 + (h[i5] - y[i5])*x3[i5]

  theta[3] = theta[3] - (alpha/m)*temp1

  print("New parameter value is: ",theta)

print("result coefficient is",theta)